{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About the Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project is majorly to conduct wrangling on tweet archive of Twitter user @dog_rates\n",
    "(https://twitter.com/dog_rates), also known as WeRateDogs. This archive/dataset consists of 2356 basic tweet data from November, 2015 to August, 2017. WeRateDogs is a Twitter account that rates people's dogs with a humorous comments.\n",
    "\n",
    "Based on the above dataset (i.e. WeRateDogs Twitter archive), another dataset is created which consists of image predictions alongside each tweet ID, image URL, and the image number that corresponded to the most confident prediction. also, additional live data would be accessed via Api to generate current learnings from the dataset.\n",
    "\n",
    "The project main objectives is as follows: \n",
    "\n",
    "• Perform data wrangling (gathering, assessing and cleaning) on provided the sources of data. \n",
    "• Store, analyze, and visualize the wrangled data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this phase, the three sources of data were gathered and imported into a Python Pandas dataframe dataframes:\n",
    "\n",
    "• The WeRateDogs Twitter archive was downloaded manually as 'twitter-archiveenhanced.csv' \n",
    "\n",
    "• The tweet image predictions was downloaded programmatically using the Requests library from a provided URL as 'image-predictions.tsv'. \n",
    "\n",
    "• Using the tweet IDs in the Twitter archive, each tweet's entire set of JSON data were assessed  using Twitter API and Python's Tweepy library and stored in a file called 'tweet_json.txt'. Each tweet's JSON data was written to its own line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of observations were made preparing the data using both Visual and Programmatic Assessment methods. The assessment identified both quality and tidiness issues as follows:\n",
    "\n",
    "###### Quality Issues\n",
    "\n",
    "• contains retweets and therefore duplicates\n",
    "\n",
    "• unnecessary html tags in source column of twitter archive in place of utility name\n",
    "\n",
    "• non descriptive column names in archive dataframe\n",
    "\n",
    "• many tweet_id(s) of twitter archive table are missing in image predictions table\n",
    "\n",
    "• missing values\n",
    "\n",
    "• incorrect/erroneous datatypes\n",
    "\n",
    "• rating_numerator column has values less than 10 as well as some very large numbers\n",
    "\n",
    "• rating_denominator column has values other than 10\n",
    "\n",
    "• erroneous dog names starting with lowercase characters (e.g. a, an, actually, by)\n",
    "\n",
    "\n",
    "###### Tidiness Issues\n",
    "\n",
    "• doggo, floofer, pupper and puppo columns should be merged into one column named \"dog_stage\"\n",
    "\n",
    "• \"dog_breed\" column should be added ; its values based on p1_conf, p2_conf, p3_conf and p1_dog, p2_dog, p3_dog columns of image predictions table\n",
    "\n",
    "• So many unnessary columns not needed for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the phase, I started by creating copies of all assessed dataframes, then merged all copy frames using a common identify \"tweet_id\" into one single file named \"master_clean\". \n",
    "\n",
    "For each quality and tidiness issue, I performed the programmatic data cleaning process in 3 stages:\n",
    "\n",
    "• Define\n",
    "\n",
    "• Code & \n",
    "\n",
    "• Test. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Upon completion of the programmatic cleaning process, I stored the master_clean DataFrame in\n",
    "twitter_archive_master.csv file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
